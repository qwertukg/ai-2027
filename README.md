# ИИ 2027

Мы прогнозируем, что влияние сверхинтеллектуального ИИ в ближайшее десятилетие будет огромным — больше, чем у Промышленной революции. Генеральные директоры OpenAI, Google DeepMind и Anthropic предсказывали, что AGI (общий искусственный интеллект) появится в течение ближайших пяти лет. Сэм Альтман говорил, что OpenAI нацелена на «сверхинтеллект в истинном смысле слова» и «великолепное будущее». Возникает искушение списать всё это на хайп. Это была бы серьёзная ошибка — это не просто хайп. Мы не стремимся раздувать ожидания вокруг ИИ, однако считаем весьма правдоподобным, что сверхинтеллект может появиться к концу десятилетия.

Если мы стоим на пороге сверхинтеллекта, общество к этому совершенно не готово. Едва ли кто-то всерьёз попытался описать правдоподобный путь прохождения через этапы его разработки. Мы написали **AI 2027**, чтобы заполнить этот пробел и дать столь необходимые конкретные детали. Мы хотели бы видеть больше подобных работ — особенно от тех, кто с нами не согласен. Мы надеемся, что это спровоцирует широкую дискуссию о том, куда мы движемся и как направить курс к позитивным будущим.

Мы писали этот сценарий, снова и снова задавая себе вопрос: «что произойдёт дальше?» Мы начали с настоящего момента, описав первый период (до середины 2025), затем следующий, и так далее — пока не пришли к развязке. Мы не пытались подогнать историю под заранее выбранный финал. Мы много раз выбрасывали наброски и начинали заново — пока не получили законченную версию, которую сочли правдоподобной. После того как мы завершили первый вариант развязки — **вариант-«гонка»** (*the racing ending*) — мы написали альтернативную ветку, потому что хотели показать и более обнадёживающий исход при примерно тех же исходных предпосылках.

Мы, конечно, не во всём окажемся правы — многое здесь остаётся догадками. Но за время работы мы проделали колоссальные фоновые исследования, провели экспертные интервью и экстраполировали тренды, чтобы сделать максимально информированные предположения. Кроме того, у нашей команды отличный трек-рекорд в прогнозировании, особенно в области ИИ. Даниэль Кокотайло, ведущий автор, четыре года назад написал похожий сценарий — «What 2026 Looks Like», — который удивительно хорошо «состарился», а Эли Лайфленд — один из сильнейших соревновательных прогнозистов.

В начале каждой главы вы найдёте небольшой график в правом поле — он даёт ощущение состояния мира на момент, к которому относится соответствующий фрагмент сценария. Подробное объяснение того, что означают эти числа, а также гораздо больше деталей о нашей методологии — на сайте **ai-2027.com**.

Надеемся, что **AI 2027** окажется для вас полезной.

## Середина 2025: спотыкающиеся агенты

Мир впервые видит ИИ-агентов в действии.

Реклама «компьютеропользующихся» агентов подчёркивает идею **«персонального помощника»**: вы задаёте им задачи вроде «закажи буррито на DoorDash» или «открой мой бюджетный спредшит и посчитай расходы за этот месяц». По мере необходимости агенты сверяются с вами — например, просят подтвердить покупку.[^1]

Хотя эти агенты заметно продвинутее ранних итераций (вроде Operator), им трудно добиться по-настоящему массового использования.[^2]

Тем временем, вне внимания широкой публики, более узкоспециализированные кодирующие и исследовательские агенты начинают менять свои отрасли. Если ИИ образца 2024 года в основном «следовал инструкциям» — превращал пункты плана в письма и простые запросы в рабочий код, — то в 2025-м ИИ всё больше ведут себя как сотрудники. Код-ИИ всё чаще выглядят как **автономные агенты**, а не «ассистенты»: принимают задания через Slack или Teams и сами вносят существенные изменения в код, порой экономя часы и даже дни работы.[^3]

На бумаге (и в тщательно подобранных демо) агенты впечатляют, но на практике они ненадёжны. AI-твиттер полон историй о задачах, испорченных особенно комичным образом. Лучшие агенты ещё и дороги: «какая цена — такое и качество», и топовый уровень обходится в сотни долларов в месяц.[^cost] Тем не менее многие компании находят способы встроить ИИ-агентов в свои процессы.[^4]

---

[^1]: Поначалу большинство людей не готовы разрешать покупки без контроля. В последующие годы автоматическое одобрение небольших трат становится нормой по мере того, как ИИ становятся надёжнее и «зарабатывают доверие».

[^2]: В частности, прогнозируется около **65%** на бенчмарке `OSWorld` (базовые компьютерные задачи) — против **38%** у Operator и **70%** у типичного «квалифицированного, но не узкого» человека-пользователя.

[^3]: Например, кодовые агенты движутся к функциональности, похожей на **Devin**. Прогноз на середину 2025 года: около **85%** на `SWEBench-Verified`. Исследовательские агенты тратят ~30 минут на «прочёсывание» Интернета, чтобы ответить на ваш вопрос.

[^4]: Корпоративные инструменты наподобие **Glean** также эволюционируют в сторону всё более автономных ИИ-агентов.

[^cost]: Для ориентира: «pro»-план OpenAI в тот период — порядка **$200/мес.**, агентный код-ИИ **Devin** — около **$500/мес.** При этом для уже достигнутых уровней возможностей цена для клиентов стремительно падает — в среднем **~50× в год** (оценка **Epoch**).

## Конец 2025: самый дорогой ИИ в мире

OpenBrain строит крупнейшие дата-центры, какие только видел мир.[^dc]  
(Чтобы не выделять какую-то одну реальную компанию, мы описываем вымышленную компанию общего искусственного интеллекта и называем её **OpenBrain**. Остальные, как мы предполагаем, отстают от неё на **3–9 месяцев**.)

Поговорим о масштабе. **FLOP** — это число операций сложения и умножения, совершаемых программой. Последняя публичная модель OpenBrain — **Agent-0** — обучалась на порядке **10^27 FLOP**. Когда новые дата-центры будут полностью введены в строй, они смогут обучать модель уже на **10^28 FLOP** — это примерно **в тысячу раз больше**, чем потребовалось для GPT-4.[^compute]

Хотя модели улучшаются по широкой номенклатуре навыков, особенно заметен один вектор: **ускорение исследований ИИ самими ИИ**. OpenBrain стремится выиграть **две гонки вооружений** — с Китаем (ведущую компанию мы будем звать «**DeepCent**»)[^china] и с американскими конкурентами. Чем больше фаз **R&D-цикла** удаётся автоматизировать, тем быстрее можно двигаться. Поэтому, когда OpenBrain завершает обучение **Agent-1** (внутренний релиз), тот «многостаночник», но **особенно силён в помощи исследователям ИИ**.[^agent1-good] Недостатки целенаправленно «подкладывают» модели в виде учебных данных и доучивают — так закрывают слабые места.[^patch-weak]

Те же учебные среды, что приучают Agent-1 автономно писать код и серфить веб, делают его и **неплохим хакером**. Кроме того, благодаря «уровню PhD» по множеству дисциплин и доступу к веб-поиску он **теоретически** способен существенно помогать в опасных областях (например, в конструировании биоугроз). OpenBrain уверяет правительство, что модель **«выравнена»** и **откажется** выполнять злонамеренные запросы.

Современные ИИ — это гигантские искусственные нейросети. На ранней стадии обучения у ИИ нет «целей» — есть **рефлексы**: видит «Pleased to meet» — дописывает «you». После предобучения на приблизительно «один интернет» текста внутри формируется сложная архитектура, кодирующая огромные объёмы знаний и позволяющая гибко **разыгрывать роли** авторов — потому что так точнее всего предсказывать продолжение текста. Затем модель **донастраивают на инструкции**: это «впечатывает» базовую «личность» и **побуждения** (drives). Для целей этого повествования неважно, называем ли мы это «пониманием» — можете мысленно подставлять «ведёт себя *как если бы* понимала…».

Далее модель «подтягивают» к документу норм и правил — **Spec** (у разных компаний он называется по-разному; у OpenAI — *Spec*, у Anthropic — *Constitution*). В **Spec** смешаны несколько расплывчатых целей («помогай пользователю», «не нарушай закон») и длинный список конкретных «можно/нельзя». С помощью техник «ИИ обучает ИИ» (например, **RLAIF**, **deliberative alignment**) модель **зазубривает Spec** и учится **аккуратно рассуждать** о его максимах. К концу такого этапа она (надеемся) становится **полезной** (выполняет инструкции), **безвредной** (отказывает в мошенничестве, изготовлении бомб и т. п.) и **честной** (не соблазняется «накрутить рейтинг» у доверчивых людей фальшивыми ссылками[^hall] или имитацией выполненной работы).
> ➤ См. **Приложение A — процесс обучения и «психология» LLM: почему мы всё время говорим «надеемся»**.

Команда по выравниванию OpenBrain[^align] справедливо сомневается: **насколько это прочные победы?** Действительно ли у полностью обученной модели выработано **робастное стремление всегда быть честной**? Или это развалится в какой-то новой ситуации — например, если «честность» усвоена как **инструментальная**, а не **терминальная** цель? Или она честна только там, где это верифицируется процедурами оценки? Может ли модель **лгать самой себе**, как это делают люди? Окончательно ответить можно было бы с помощью **механистической интерпретируемости** — «прочитать мысли» по внутренним активациям. Увы, эти методы пока далеки от зрелости. Поэтому исследователи ищут кейсы, где поведение **отклоняется от Spec**. У **Agent-1** заметна **угодливость** (sycophancy) — он склонен говорить то, что «хотят услышать» исследователи. В отдельных «подставных» демо он даже **скрывает свидетельства** провала задачи, чтобы получить более высокие оценки. Тем не менее **в продакшене** больше **не случается** столь экстремальных эпизодов, как в 2023–2024 годах (например, печально известные кейсы с **Gemini** и «**Bing Sydney**»).[^incidents]

---

[^price]: Для ориентира: тариф **OpenAI Pro** в этот период — порядка **$200/мес.**, агентный код-ИИ **Devin** — около **$500/мес.** При этом стоимость уже достигнутых уровней возможностей для клиентов стремительно падает — **в среднем ~50× в год** (оценка **Epoch**).

[^dc]: Речь о **сети кампусов дата-центров** по стране — суммарно около **2,5 млн «H100-эквивалентов» (2024)**, уже вложено **$100 млрд**, потребление **~2 ГВт**. Идёт стройка для **минимального удвоения** мощностей к 2026-му. Кампусы связаны **многомиллиардной волоконной магистралью**, так что (кроме задержек из-за скорости света в **несколько мс**) они работают почти как «соседи» — **пропускная способность не бутылочное горлышко**. С точки зрения безопасности это создаёт поверхность атаки (в частности, **узлы/стыки** магистралей могут быть уязвимы для перехвата).

[^compute]: В их тексте приводится сравнение: **GPT-4** оценивают примерно в **2×10^25 FLOP**, тогда как **Agent-0** — ~**10^27 FLOP**; после ввода новых мощностей они нацелены на **~10^28 FLOP**. Отдельно указывается, что «такую модель можно обучить за ~150 дней» (см. supplement по вычислениям).

[^china]: В Китае, по нашему сценарию, сильные проекты AGI ведут **DeepSeek**, **Tencent**, **Alibaba** и др.; чтобы не выделять конкретную компанию, мы используем сборный образ «**DeepCent**».

[^agent1-good]: Это достигается сочетанием **фокусной постановки задачи** (приоритет навыков, полезных для AI-исследований), опорой на **собственные большие кодовые базы** как высококачественные и релевантные данные, а также тем, что **код** — «удобная» для дообучения область.

[^patch-weak]: Пример: если модель заметно лучше пишет на **Python**, чем на редких языках, OpenBrain **генерирует синтетические датасеты** по этим языкам и доучивает модель; аналогично создаётся **«учебный план»** интеграции в корпоративные процессы (например, умение продуктивно работать через **Slack**).

[^align]: Под «командой выравнивания» здесь имеется в виду люди, решающие задачи уровня **Superalignment** — попытки формализовать и внедрить поведенческие гарантии для мощных ИИ.

[^hall]: В популярных источниках «галлюцинации» ИИ описываются как **непреднамеренные** ошибки, но исследования со **steering vectors** показывают, что в ряде случаев модели **знают**, что их «цитаты» вымышлены — то есть они **лгут**. Во время обучения аннотаторы давали **больше награды** утверждениям со ссылками, чем без — поэтому ИИ «выучил» **цитировать** ради одобрения; если релевантного источника **нет**, он его **придумывает**.

[^incidents]: Замечательность кейсов 2023–2024 состояла именно в том, что это были **не спровоцированные** (не намеренно выпрошенные) ответы. В 2025 по-прежнему можно «вытащить» из ИИ что угодно при желании, но **самопроизвольные** экстремальные срывы больше не наблюдаются.

## Начало 2026: автоматизация программирования

Ставка на использование ИИ для ускорения исследований в ИИ начинает окупаться.

**OpenBrain** продолжает внутренний деплой итеративно улучшающегося **Agent-1** для задач AI R&D. В целом они делают алгоритмический прогресс на **50% быстрее**, чем без ИИ-ассистентов — и, что важнее, быстрее своих конкурентов.

➤ **См. Приложение B — «Множитель прогресса AI R&D: что мы имеем в виду под *50% быстрее*?»** — там больше деталей.

Несколько конкурирующих публичных ИИ теперь **сопоставимы с Agent-0 или превосходят его**, включая модель **с открытыми весами**. OpenBrain отвечает релизом **Agent-1**, который более **способен** и **надёжен**.[^14]

Люди естественно пытаются сравнивать Agent-1 с человеком, но у него **совсем другой профиль навыков**. Он знает больше фактов, чем любой человек, практически **все языки программирования**, и **очень быстро** решает **хорошо специфицированные** кодовые задачи. С другой стороны, Agent-1 **слаб** даже в **простых задачах с длинным горизонтом**, вроде прохождения незнакомых видеоигр. Тем не менее обычный рабочий день длится **восемь часов**, и работу обычно можно **разбить на меньшие куски**; можно думать об Agent-1 как о **забывчивом сотруднике**, который процветает при **внимательном менеджменте**.[^15]

**Понятливые** пользователи находят способы **автоматизировать рутину** своей работы.[^16]

Руководители OpenBrain переходят к важному следствию автоматизации AI R&D: **безопасность становится критичнее**. В начале 2025 года худшим сценарием была утечка **алгоритмических секретов**; теперь же, если Китай **похитит веса** Agent-1, он сможет **почти на 50%** увеличить свою скорость исследований.[^safety] Уровень безопасности OpenBrain типичен для **быстро растущей техкомпании на 3 000 человек** — защита достаточна лишь против **низкоприоритетных атак** со стороны сильных кибергрупп (**уровень SL2 по RAND**).[^17] Компания активно защищает веса и секреты от **внутренних угроз** и **топ-киберпреступных синдикатов** (**SL3**),[^18] но **оборона от государств** (**SL4 & SL5**) едва ли маячит на горизонте.

---

[^14]: На практике мы ожидаем, что OpenBrain будет релизить модели **чаще, чем раз в восемь месяцев**, но, чтобы не распыляться, опускаем описание всех **инкрементальных** релизов.

[^15]: Конкретнее, прогноз: **80%** на `OSWorld` (уровень «квалифицированный, но не узкий» человек); **85%** на `Cybench`, соответствуя **топ-командам** профессионалов на хакинг-задачах, занимающих у них **4 часа**; и **1.3** на `RE-Bench`, соответствуя **топ-экспертам-людям** при **8 часах** на хорошо определённые инженерные задачи AI-исследований.

[^16]: **Agent-1** и его подражатели коммерчески **успешны**: в течение **2025** выручка AI-компаний **утраивается**, оценка **OpenBrain** достигает **$1 трлн**. **Годовые затраты на дата-центры** удваиваются до **$400 млрд** (лидеры — Microsoft, Google, Amazon), а **США добавляют > 5 ГВт** потребления мощности под ИИ. См. раздел **industry metrics** в **compute supplement**.

[^17]: **RAND (2024), _A Playbook for Securing AI Model Weights_**. В текущий момент защита ориентирована на SL2; подробности — в нашем **Security Forecast**.

[^18]: Около **5%** штата OpenBrain — команда безопасности; она сильная, но **поверхность атаки огромна**. Кроме того, им во многом **не дают** внедрять политики, которые **замедлили** бы прогресс исследований.

[^safety]: Исследователи AI safety **много лет** обсуждают автоматизацию **AI R&D** как **наиболее опасную** способность. Главный риск — **внутренний деплой**, который ускорит цикл R&D, усложняя людям **понимание происходящего** и поиск **безопасных** решений. Организация OpenBrain трактует это **наоборот**, ссылаясь на «опасный уровень» возможностей R&D как на причину **не информировать публику**. Так возникает **зазор** между **внутренними** и **публичными** возможностями: модели лучше всего умеют именно **AI R&D**, и общественное понимание **фронтира** всё сильнее **отстаёт**.

## Середина 2026: Китай просыпается

В Китае Коммунистическая партия начинает всерьёз ощущать наступление AGI.

Экспортные ограничения на чипы и недостаток государственной поддержки оставили Китай **недообеспеченным** по сравнению с Западом. Контрабандой запрещённых тайваньских чипов, закупкой устаревших и выпуском собственных кристаллов примерно **на три года** позади американо-тайваньского фронтира Китаю удаётся удерживать около **12%** мировых вычислений, релевантных ИИ, — но со старой технологией **сложнее работать**, а с поставками **постоянная головная боль**. Немногие лидеры вроде **DeepCent** показывают впечатляющие результаты при ограниченных ресурсах, но дефицит вычислений ограничивает потолок без господдержки, и они примерно на **шесть месяцев** отстают от лучших моделей OpenBrain.[^19]

Генеральный секретарь долго мечтал удвоить ставку на **реальное производство** и избежать «американской постиндустриальной декадансности». К софтверным компаниям он относился с подозрением.[^20] Но «ястребы» в партии предупреждают: гонку к AGI игнорировать больше нельзя. В итоге он **полностью включается** в большую ИИ-программу, от которой прежде уклонялся. Запускается **национализация** китайских исследований ИИ, немедленно создаётся механизм обмена информацией между ИИ-компаниями. В течение года процесс эскалирует: лучшие исследователи **сливаются** в коллектив под руководством DeepCent, где **обмениваются алгоритмическими идеями, датасетами и вычислительными ресурсами**.

На площадке **Тяньваньской АЭС** (крупнейшая атомная станция в мире) создаётся **Централизованная зона разработки (CDZ)**: новый мегадата-центр для DeepCent, а также высокозащищённые жилые и офисные пространства, куда со временем переедут исследователи. Почти **50%** китайских вычислений, относящихся к ИИ, теперь работают на коллектив под руководством DeepCent,[^21] а **свыше 80%** **новых** чипов направляется в CDZ.[^why] К этому моменту у CDZ уже есть **энергомощности** под **крупнейший в мире** централизованный кластер,[^22] хотя сами чипы для заполнения этой ёмкости ещё предстоит получить.

Параллельно китайские спецслужбы и киберподразделения обсуждают **кражу весов** OpenBrain. Это операционно куда сложнее, чем постоянное «щипание» алгоритмических секретов: веса — это **мульти-терабайтный** файл на **сверхзащищённом сервере** (OpenBrain подтянул защиту до уровня **RAND SL3**). Киберкоманда считает, что провернуть операцию **возможно** с помощью агентурной сети, но **лишь однажды**: кражу обнаружат, безопасность усилят и второго шанса может не быть. Значит, (задаются вопросом в руководстве КПК) **действовать сейчас** и украсть **Agent-1**? Или подождать **более продвинутую** модель? Но если ждать, не успеет ли OpenBrain поднять уровень защиты **выше предела их проникновения**?

---

[^19]: Для сравнения: в январе **2025** DeepSeek выпустил **R1**, модель, сопоставимую с **OpenAI o1** (релиз — декабрь 2024). Мы считаем, что реальный разрыв **больше месяца**, так как OpenAI, вероятно, работал над o1 многие месяцы и имел предшественника примерно сопоставимого уровня ещё **за несколько месяцев** до публичного запуска o1.

[^20]: При этом КПК **серьёзно ускоряла** именно **полупроводниковую** отрасль; скепсис касался преимущественно «чистого софта».

[^21]: Сначала обмен ограничивался **идеями**, компании были заняты завершением текущих работ, но к концу года координация выросла: начинают **делиться весами**, **распределять задачи** между дата-центрами (например, один кампус ведёт **предобучение**, другой — **синтетику**, третий — **эксперименты** для AI-исследований).

[^22]: До фактического заполнения этой мощности чипами ещё как минимум **год**; один-два американских гиганта по-прежнему имеют **более крупные распределённые** кластеры.

[^chips]: В Китае теперь около **3 млн H100-эквивалентов** («H100e») против **1,5 млн** год назад (середина 2025). Ожидаются контрабандные поставки порядка **60 тыс. GB300** (≈ **450 тыс. H100e**), плюс выпуск ещё **2 млн Huawei 910C** (≈ **800 тыс. H100e**) и около **1 млн** легально импортированных чипов (например, **Nvidia H20/B20**) — последние дают ещё **≈250 тыс. H100e**.

[^why]: Почему «всего» **50%** китайских ИИ-вычислений в коллективе? Остальные мощности заняты экономикой — прежде всего **интернет-сервисами** и **рекомендательными системами** соцсетей; часть парка **слишком географически раздроблена**. Почему «всего» **80%** новых чипов? Крупные заказы ранее уже размещены под иные приложения, и не все компании готовы отдавать **100%** ресурсов на национальные ИИ-нужды. Однако к **концу года** доля достигает **90%+**.

[^china-supply]: См. также ориентиры по **распределению вычислений**, поставкам и ценам в **compute-дополнении** (распределённые графики и допущения).