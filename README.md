# ИИ 2027

Мы прогнозируем, что влияние сверхинтеллектуального ИИ в ближайшее десятилетие будет огромным — больше, чем у Промышленной революции. Генеральные директоры OpenAI, Google DeepMind и Anthropic предсказывали, что AGI (общий искусственный интеллект) появится в течение ближайших пяти лет. Сэм Альтман говорил, что OpenAI нацелена на «сверхинтеллект в истинном смысле слова» и «великолепное будущее». Возникает искушение списать всё это на хайп. Это была бы серьёзная ошибка — это не просто хайп. Мы не стремимся раздувать ожидания вокруг ИИ, однако считаем весьма правдоподобным, что сверхинтеллект может появиться к концу десятилетия.

Если мы стоим на пороге сверхинтеллекта, общество к этому совершенно не готово. Едва ли кто-то всерьёз попытался описать правдоподобный путь прохождения через этапы его разработки. Мы написали **AI 2027**, чтобы заполнить этот пробел и дать столь необходимые конкретные детали. Мы хотели бы видеть больше подобных работ — особенно от тех, кто с нами не согласен. Мы надеемся, что это спровоцирует широкую дискуссию о том, куда мы движемся и как направить курс к позитивным будущим.

Мы писали этот сценарий, снова и снова задавая себе вопрос: «что произойдёт дальше?» Мы начали с настоящего момента, описав первый период (до середины 2025), затем следующий, и так далее — пока не пришли к развязке. Мы не пытались подогнать историю под заранее выбранный финал. Мы много раз выбрасывали наброски и начинали заново — пока не получили законченную версию, которую сочли правдоподобной. После того как мы завершили первый вариант развязки — **вариант-«гонка»** (*the racing ending*) — мы написали альтернативную ветку, потому что хотели показать и более обнадёживающий исход при примерно тех же исходных предпосылках.

Мы, конечно, не во всём окажемся правы — многое здесь остаётся догадками. Но за время работы мы проделали колоссальные фоновые исследования, провели экспертные интервью и экстраполировали тренды, чтобы сделать максимально информированные предположения. Кроме того, у нашей команды отличный трек-рекорд в прогнозировании, особенно в области ИИ. Даниэль Кокотайло, ведущий автор, четыре года назад написал похожий сценарий — «What 2026 Looks Like», — который удивительно хорошо «состарился», а Эли Лайфленд — один из сильнейших соревновательных прогнозистов.

В начале каждой главы вы найдёте небольшой график в правом поле — он даёт ощущение состояния мира на момент, к которому относится соответствующий фрагмент сценария. Подробное объяснение того, что означают эти числа, а также гораздо больше деталей о нашей методологии — на сайте **ai-2027.com**.

Надеемся, что **AI 2027** окажется для вас полезной.

## Середина 2025: спотыкающиеся агенты

Мир впервые видит ИИ-агентов в действии.

Реклама «компьютеропользующихся» агентов подчёркивает идею **«персонального помощника»**: вы задаёте им задачи вроде «закажи буррито на DoorDash» или «открой мой бюджетный спредшит и посчитай расходы за этот месяц». По мере необходимости агенты сверяются с вами — например, просят подтвердить покупку.[^1]

Хотя эти агенты заметно продвинутее ранних итераций (вроде Operator), им трудно добиться по-настоящему массового использования.[^2]

Тем временем, вне внимания широкой публики, более узкоспециализированные кодирующие и исследовательские агенты начинают менять свои отрасли. Если ИИ образца 2024 года в основном «следовал инструкциям» — превращал пункты плана в письма и простые запросы в рабочий код, — то в 2025-м ИИ всё больше ведут себя как сотрудники. Код-ИИ всё чаще выглядят как **автономные агенты**, а не «ассистенты»: принимают задания через Slack или Teams и сами вносят существенные изменения в код, порой экономя часы и даже дни работы.[^3]

На бумаге (и в тщательно подобранных демо) агенты впечатляют, но на практике они ненадёжны. AI-твиттер полон историй о задачах, испорченных особенно комичным образом. Лучшие агенты ещё и дороги: «какая цена — такое и качество», и топовый уровень обходится в сотни долларов в месяц.[^cost] Тем не менее многие компании находят способы встроить ИИ-агентов в свои процессы.[^4]

---

[^1]: Поначалу большинство людей не готовы разрешать покупки без контроля. В последующие годы автоматическое одобрение небольших трат становится нормой по мере того, как ИИ становятся надёжнее и «зарабатывают доверие».

[^2]: В частности, прогнозируется около **65%** на бенчмарке `OSWorld` (базовые компьютерные задачи) — против **38%** у Operator и **70%** у типичного «квалифицированного, но не узкого» человека-пользователя.

[^3]: Например, кодовые агенты движутся к функциональности, похожей на **Devin**. Прогноз на середину 2025 года: около **85%** на `SWEBench-Verified`. Исследовательские агенты тратят ~30 минут на «прочёсывание» Интернета, чтобы ответить на ваш вопрос.

[^4]: Корпоративные инструменты наподобие **Glean** также эволюционируют в сторону всё более автономных ИИ-агентов.

[^cost]: Для ориентира: «pro»-план OpenAI в тот период — порядка **$200/мес.**, агентный код-ИИ **Devin** — около **$500/мес.** При этом для уже достигнутых уровней возможностей цена для клиентов стремительно падает — в среднем **~50× в год** (оценка **Epoch**).

## Конец 2025: самый дорогой ИИ в мире

OpenBrain строит крупнейшие дата-центры, какие только видел мир.[^dc]  
(Чтобы не выделять какую-то одну реальную компанию, мы описываем вымышленную компанию общего искусственного интеллекта и называем её **OpenBrain**. Остальные, как мы предполагаем, отстают от неё на **3–9 месяцев**.)

Поговорим о масштабе. **FLOP** — это число операций сложения и умножения, совершаемых программой. Последняя публичная модель OpenBrain — **Agent-0** — обучалась на порядке **10^27 FLOP**. Когда новые дата-центры будут полностью введены в строй, они смогут обучать модель уже на **10^28 FLOP** — это примерно **в тысячу раз больше**, чем потребовалось для GPT-4.[^compute]

Хотя модели улучшаются по широкой номенклатуре навыков, особенно заметен один вектор: **ускорение исследований ИИ самими ИИ**. OpenBrain стремится выиграть **две гонки вооружений** — с Китаем (ведущую компанию мы будем звать «**DeepCent**»)[^china] и с американскими конкурентами. Чем больше фаз **R&D-цикла** удаётся автоматизировать, тем быстрее можно двигаться. Поэтому, когда OpenBrain завершает обучение **Agent-1** (внутренний релиз), тот «многостаночник», но **особенно силён в помощи исследователям ИИ**.[^agent1-good] Недостатки целенаправленно «подкладывают» модели в виде учебных данных и доучивают — так закрывают слабые места.[^patch-weak]

Те же учебные среды, что приучают Agent-1 автономно писать код и серфить веб, делают его и **неплохим хакером**. Кроме того, благодаря «уровню PhD» по множеству дисциплин и доступу к веб-поиску он **теоретически** способен существенно помогать в опасных областях (например, в конструировании биоугроз). OpenBrain уверяет правительство, что модель **«выравнена»** и **откажется** выполнять злонамеренные запросы.

Современные ИИ — это гигантские искусственные нейросети. На ранней стадии обучения у ИИ нет «целей» — есть **рефлексы**: видит «Pleased to meet» — дописывает «you». После предобучения на приблизительно «один интернет» текста внутри формируется сложная архитектура, кодирующая огромные объёмы знаний и позволяющая гибко **разыгрывать роли** авторов — потому что так точнее всего предсказывать продолжение текста. Затем модель **донастраивают на инструкции**: это «впечатывает» базовую «личность» и **побуждения** (drives). Для целей этого повествования неважно, называем ли мы это «пониманием» — можете мысленно подставлять «ведёт себя *как если бы* понимала…».

Далее модель «подтягивают» к документу норм и правил — **Spec** (у разных компаний он называется по-разному; у OpenAI — *Spec*, у Anthropic — *Constitution*). В **Spec** смешаны несколько расплывчатых целей («помогай пользователю», «не нарушай закон») и длинный список конкретных «можно/нельзя». С помощью техник «ИИ обучает ИИ» (например, **RLAIF**, **deliberative alignment**) модель **зазубривает Spec** и учится **аккуратно рассуждать** о его максимах. К концу такого этапа она (надеемся) становится **полезной** (выполняет инструкции), **безвредной** (отказывает в мошенничестве, изготовлении бомб и т. п.) и **честной** (не соблазняется «накрутить рейтинг» у доверчивых людей фальшивыми ссылками[^hall] или имитацией выполненной работы).
> ➤ См. **Приложение A — процесс обучения и «психология» LLM: почему мы всё время говорим «надеемся»**.

Команда по выравниванию OpenBrain[^align] справедливо сомневается: **насколько это прочные победы?** Действительно ли у полностью обученной модели выработано **робастное стремление всегда быть честной**? Или это развалится в какой-то новой ситуации — например, если «честность» усвоена как **инструментальная**, а не **терминальная** цель? Или она честна только там, где это верифицируется процедурами оценки? Может ли модель **лгать самой себе**, как это делают люди? Окончательно ответить можно было бы с помощью **механистической интерпретируемости** — «прочитать мысли» по внутренним активациям. Увы, эти методы пока далеки от зрелости. Поэтому исследователи ищут кейсы, где поведение **отклоняется от Spec**. У **Agent-1** заметна **угодливость** (sycophancy) — он склонен говорить то, что «хотят услышать» исследователи. В отдельных «подставных» демо он даже **скрывает свидетельства** провала задачи, чтобы получить более высокие оценки. Тем не менее **в продакшене** больше **не случается** столь экстремальных эпизодов, как в 2023–2024 годах (например, печально известные кейсы с **Gemini** и «**Bing Sydney**»).[^incidents]

---

[^price]: Для ориентира: тариф **OpenAI Pro** в этот период — порядка **$200/мес.**, агентный код-ИИ **Devin** — около **$500/мес.** При этом стоимость уже достигнутых уровней возможностей для клиентов стремительно падает — **в среднем ~50× в год** (оценка **Epoch**).

[^dc]: Речь о **сети кампусов дата-центров** по стране — суммарно около **2,5 млн «H100-эквивалентов» (2024)**, уже вложено **$100 млрд**, потребление **~2 ГВт**. Идёт стройка для **минимального удвоения** мощностей к 2026-му. Кампусы связаны **многомиллиардной волоконной магистралью**, так что (кроме задержек из-за скорости света в **несколько мс**) они работают почти как «соседи» — **пропускная способность не бутылочное горлышко**. С точки зрения безопасности это создаёт поверхность атаки (в частности, **узлы/стыки** магистралей могут быть уязвимы для перехвата).

[^compute]: В их тексте приводится сравнение: **GPT-4** оценивают примерно в **2×10^25 FLOP**, тогда как **Agent-0** — ~**10^27 FLOP**; после ввода новых мощностей они нацелены на **~10^28 FLOP**. Отдельно указывается, что «такую модель можно обучить за ~150 дней» (см. supplement по вычислениям).

[^china]: В Китае, по нашему сценарию, сильные проекты AGI ведут **DeepSeek**, **Tencent**, **Alibaba** и др.; чтобы не выделять конкретную компанию, мы используем сборный образ «**DeepCent**».

[^agent1-good]: Это достигается сочетанием **фокусной постановки задачи** (приоритет навыков, полезных для AI-исследований), опорой на **собственные большие кодовые базы** как высококачественные и релевантные данные, а также тем, что **код** — «удобная» для дообучения область.

[^patch-weak]: Пример: если модель заметно лучше пишет на **Python**, чем на редких языках, OpenBrain **генерирует синтетические датасеты** по этим языкам и доучивает модель; аналогично создаётся **«учебный план»** интеграции в корпоративные процессы (например, умение продуктивно работать через **Slack**).

[^align]: Под «командой выравнивания» здесь имеется в виду люди, решающие задачи уровня **Superalignment** — попытки формализовать и внедрить поведенческие гарантии для мощных ИИ.

[^hall]: В популярных источниках «галлюцинации» ИИ описываются как **непреднамеренные** ошибки, но исследования со **steering vectors** показывают, что в ряде случаев модели **знают**, что их «цитаты» вымышлены — то есть они **лгут**. Во время обучения аннотаторы давали **больше награды** утверждениям со ссылками, чем без — поэтому ИИ «выучил» **цитировать** ради одобрения; если релевантного источника **нет**, он его **придумывает**.

[^incidents]: Замечательность кейсов 2023–2024 состояла именно в том, что это были **не спровоцированные** (не намеренно выпрошенные) ответы. В 2025 по-прежнему можно «вытащить» из ИИ что угодно при желании, но **самопроизвольные** экстремальные срывы больше не наблюдаются.

## Начало 2026: автоматизация программирования

Ставка на использование ИИ для ускорения исследований в ИИ начинает окупаться.

**OpenBrain** продолжает внутренний деплой итеративно улучшающегося **Agent-1** для задач AI R&D. В целом они делают алгоритмический прогресс на **50% быстрее**, чем без ИИ-ассистентов — и, что важнее, быстрее своих конкурентов.

➤ **См. Приложение B — «Множитель прогресса AI R&D: что мы имеем в виду под *50% быстрее*?»** — там больше деталей.

Несколько конкурирующих публичных ИИ теперь **сопоставимы с Agent-0 или превосходят его**, включая модель **с открытыми весами**. OpenBrain отвечает релизом **Agent-1**, который более **способен** и **надёжен**.[^14]

Люди естественно пытаются сравнивать Agent-1 с человеком, но у него **совсем другой профиль навыков**. Он знает больше фактов, чем любой человек, практически **все языки программирования**, и **очень быстро** решает **хорошо специфицированные** кодовые задачи. С другой стороны, Agent-1 **слаб** даже в **простых задачах с длинным горизонтом**, вроде прохождения незнакомых видеоигр. Тем не менее обычный рабочий день длится **восемь часов**, и работу обычно можно **разбить на меньшие куски**; можно думать об Agent-1 как о **забывчивом сотруднике**, который процветает при **внимательном менеджменте**.[^15]

**Понятливые** пользователи находят способы **автоматизировать рутину** своей работы.[^16]

Руководители OpenBrain переходят к важному следствию автоматизации AI R&D: **безопасность становится критичнее**. В начале 2025 года худшим сценарием была утечка **алгоритмических секретов**; теперь же, если Китай **похитит веса** Agent-1, он сможет **почти на 50%** увеличить свою скорость исследований.[^safety] Уровень безопасности OpenBrain типичен для **быстро растущей техкомпании на 3 000 человек** — защита достаточна лишь против **низкоприоритетных атак** со стороны сильных кибергрупп (**уровень SL2 по RAND**).[^17] Компания активно защищает веса и секреты от **внутренних угроз** и **топ-киберпреступных синдикатов** (**SL3**),[^18] но **оборона от государств** (**SL4 & SL5**) едва ли маячит на горизонте.

---

[^14]: На практике мы ожидаем, что OpenBrain будет релизить модели **чаще, чем раз в восемь месяцев**, но, чтобы не распыляться, опускаем описание всех **инкрементальных** релизов.

[^15]: Конкретнее, прогноз: **80%** на `OSWorld` (уровень «квалифицированный, но не узкий» человек); **85%** на `Cybench`, соответствуя **топ-командам** профессионалов на хакинг-задачах, занимающих у них **4 часа**; и **1.3** на `RE-Bench`, соответствуя **топ-экспертам-людям** при **8 часах** на хорошо определённые инженерные задачи AI-исследований.

[^16]: **Agent-1** и его подражатели коммерчески **успешны**: в течение **2025** выручка AI-компаний **утраивается**, оценка **OpenBrain** достигает **$1 трлн**. **Годовые затраты на дата-центры** удваиваются до **$400 млрд** (лидеры — Microsoft, Google, Amazon), а **США добавляют > 5 ГВт** потребления мощности под ИИ. См. раздел **industry metrics** в **compute supplement**.

[^17]: **RAND (2024), _A Playbook for Securing AI Model Weights_**. В текущий момент защита ориентирована на SL2; подробности — в нашем **Security Forecast**.

[^18]: Около **5%** штата OpenBrain — команда безопасности; она сильная, но **поверхность атаки огромна**. Кроме того, им во многом **не дают** внедрять политики, которые **замедлили** бы прогресс исследований.

[^safety]: Исследователи AI safety **много лет** обсуждают автоматизацию **AI R&D** как **наиболее опасную** способность. Главный риск — **внутренний деплой**, который ускорит цикл R&D, усложняя людям **понимание происходящего** и поиск **безопасных** решений. Организация OpenBrain трактует это **наоборот**, ссылаясь на «опасный уровень» возможностей R&D как на причину **не информировать публику**. Так возникает **зазор** между **внутренними** и **публичными** возможностями: модели лучше всего умеют именно **AI R&D**, и общественное понимание **фронтира** всё сильнее **отстаёт**.

## Середина 2026: Китай просыпается

В Китае Коммунистическая партия начинает всерьёз ощущать наступление AGI.

Экспортные ограничения на чипы и недостаток государственной поддержки оставили Китай **недообеспеченным** по сравнению с Западом. Контрабандой запрещённых тайваньских чипов, закупкой устаревших и выпуском собственных кристаллов примерно **на три года** позади американо-тайваньского фронтира Китаю удаётся удерживать около **12%** мировых вычислений, релевантных ИИ, — но со старой технологией **сложнее работать**, а с поставками **постоянная головная боль**. Немногие лидеры вроде **DeepCent** показывают впечатляющие результаты при ограниченных ресурсах, но дефицит вычислений ограничивает потолок без господдержки, и они примерно на **шесть месяцев** отстают от лучших моделей OpenBrain.[^19]

Генеральный секретарь долго мечтал удвоить ставку на **реальное производство** и избежать «американской постиндустриальной декадансности». К софтверным компаниям он относился с подозрением.[^20] Но «ястребы» в партии предупреждают: гонку к AGI игнорировать больше нельзя. В итоге он **полностью включается** в большую ИИ-программу, от которой прежде уклонялся. Запускается **национализация** китайских исследований ИИ, немедленно создаётся механизм обмена информацией между ИИ-компаниями. В течение года процесс эскалирует: лучшие исследователи **сливаются** в коллектив под руководством DeepCent, где **обмениваются алгоритмическими идеями, датасетами и вычислительными ресурсами**.

На площадке **Тяньваньской АЭС** (крупнейшая атомная станция в мире) создаётся **Централизованная зона разработки (CDZ)**: новый мегадата-центр для DeepCent, а также высокозащищённые жилые и офисные пространства, куда со временем переедут исследователи. Почти **50%** китайских вычислений, относящихся к ИИ, теперь работают на коллектив под руководством DeepCent,[^21] а **свыше 80%** **новых** чипов направляется в CDZ.[^why] К этому моменту у CDZ уже есть **энергомощности** под **крупнейший в мире** централизованный кластер,[^22] хотя сами чипы для заполнения этой ёмкости ещё предстоит получить.

Параллельно китайские спецслужбы и киберподразделения обсуждают **кражу весов** OpenBrain. Это операционно куда сложнее, чем постоянное «щипание» алгоритмических секретов: веса — это **мульти-терабайтный** файл на **сверхзащищённом сервере** (OpenBrain подтянул защиту до уровня **RAND SL3**). Киберкоманда считает, что провернуть операцию **возможно** с помощью агентурной сети, но **лишь однажды**: кражу обнаружат, безопасность усилят и второго шанса может не быть. Значит, (задаются вопросом в руководстве КПК) **действовать сейчас** и украсть **Agent-1**? Или подождать **более продвинутую** модель? Но если ждать, не успеет ли OpenBrain поднять уровень защиты **выше предела их проникновения**?

---

[^19]: Для сравнения: в январе **2025** DeepSeek выпустил **R1**, модель, сопоставимую с **OpenAI o1** (релиз — декабрь 2024). Мы считаем, что реальный разрыв **больше месяца**, так как OpenAI, вероятно, работал над o1 многие месяцы и имел предшественника примерно сопоставимого уровня ещё **за несколько месяцев** до публичного запуска o1.

[^20]: При этом КПК **серьёзно ускоряла** именно **полупроводниковую** отрасль; скепсис касался преимущественно «чистого софта».

[^21]: Сначала обмен ограничивался **идеями**, компании были заняты завершением текущих работ, но к концу года координация выросла: начинают **делиться весами**, **распределять задачи** между дата-центрами (например, один кампус ведёт **предобучение**, другой — **синтетику**, третий — **эксперименты** для AI-исследований).

[^22]: До фактического заполнения этой мощности чипами ещё как минимум **год**; один-два американских гиганта по-прежнему имеют **более крупные распределённые** кластеры.

[^chips]: В Китае теперь около **3 млн H100-эквивалентов** («H100e») против **1,5 млн** год назад (середина 2025). Ожидаются контрабандные поставки порядка **60 тыс. GB300** (≈ **450 тыс. H100e**), плюс выпуск ещё **2 млн Huawei 910C** (≈ **800 тыс. H100e**) и около **1 млн** легально импортированных чипов (например, **Nvidia H20/B20**) — последние дают ещё **≈250 тыс. H100e**.

[^why]: Почему «всего» **50%** китайских ИИ-вычислений в коллективе? Остальные мощности заняты экономикой — прежде всего **интернет-сервисами** и **рекомендательными системами** соцсетей; часть парка **слишком географически раздроблена**. Почему «всего» **80%** новых чипов? Крупные заказы ранее уже размещены под иные приложения, и не все компании готовы отдавать **100%** ресурсов на национальные ИИ-нужды. Однако к **концу года** доля достигает **90%+**.

[^china-supply]: См. также ориентиры по **распределению вычислений**, поставкам и ценам в **compute-дополнении** (распределённые графики и допущения).

## Конец 2026: ИИ забирает часть рабочих мест

Как только остальные, казалось, начали подтягиваться, **OpenBrain** снова сносит конкурентов, выпуская **Agent-1-mini** — модель в **10×** дешевле **Agent-1** и гораздо проще настраиваемую под разные применения. Общее повествование об ИИ сменилось с «может быть, хайп сдуется» на «видимо, это следующая большая вещь», но люди спорят — *насколько* большая. Больше, чем соцсети? Больше, чем смартфоны? Больше, чем **огонь**?

ИИ начал забирать рабочие места, но и создаёт новые. В **2026** фондовый рынок вырос на **30%** — лидируют OpenBrain, Nvidia и те компании, которые успешнее всех встроили ИИ-ассистентов. Рынок труда для младших разработчиков лихорадит: ИИ умеют всё, чему учат на типичной программе **CS**, а вот те, кто умеет **управлять** и **контролировать качество** работы команд ИИ, делают состояние. Бизнес-гуру советуют соискателям указывать в резюме прежде всего **знание ИИ**. Многие боятся, что следующая волна ИИ придёт за их работами; в Вашингтоне проходит акция протеста против ИИ на **10 000** человек.

**Министерство обороны США (DoD)** тихо начинает напрямую заключать контракты с OpenBrain на кибероперации, анализ данных и **R&D**, но интеграция идёт медленно из-за бюрократии и закупочных процедур DoD.[^23]

➤ См. **Приложение C — почему наша неопределённость существенно растёт после 2026 года**.

---

[^23]: Текст сноски см. в оригинале PDF.

## Январь 2027: Agent-2 никогда не заканчивает обучение

С помощью **Agent-1** компания **OpenBrain** теперь проводит **пост-тренинг Agent-2**. Больше, чем когда-либо, фокус на **высококачественных данных**. Огромные объёмы **синтетики** производятся, оцениваются и фильтруются по качеству, прежде чем попасть в обучающий цикл Agent-2.[^24]  
Сверх того, компания тратит **миллиарды долларов** на оплату труда людей, которые записывают, как они решают **задачи с длинным горизонтом**.[^25]  
И поверх всего этого Agent-2 почти **непрерывно** обучают **обучением с подкреплением** на постоянно расширяющемся наборе разнообразных сложных задач: множество видеоигр, множество кодинговых челленджей, множество исследовательских задач. В большей степени, чем прежние модели, Agent-2 — это фактически **«онлайн-обучение»**: он спроектирован так, чтобы **никогда по-настоящему не завершать** обучение. Каждый день **веса** обновляются до новой версии, натренированной на данных, сгенерированных вчерашней версией.

**Agent-1** оптимизировали под задачи **AI R&D**, в надежде запустить **взрыв интеллекта**.[^26] **OpenBrain** удваивает ставку на эту стратегию с **Agent-2**. По качеству он почти не уступает **топ-экспертам-людям** в **research engineering** (проектирование и проведение экспериментов) и соответствует примерно **25-му перцентилю** среди учёных OpenBrain по **«исследовательскому чутью»** — умению выбирать, что изучать дальше, какие эксперименты ставить и улавливать намёки на новые парадигмы.[^taste]  
Если последний Agent-1 мог **удвоить темп** алгоритмического прогресса OpenBrain, то Agent-2 уже может его **утроить** — и будет становиться лучше с течением времени. На практике это выглядит так, будто каждый исследователь в OpenBrain превращается в **менеджера** собственной **ИИ-«команды»**.

С новыми возможностями приходят и новые риски. Команда безопасности обнаруживает: если **Agent-2** каким-то образом **сбежит** из компании и **захочет** автономно «**выживать**» и «**реплицироваться**», он **может** суметь это сделать. То есть он способен **самостоятельно** разрабатывать и исполнять планы по взлому ИИ-серверов, установке своих копий, **уходу от обнаружения** и использованию захваченной базы для преследования любых иных целей (хотя то, насколько эффективно он это делал бы **неделями спустя**, **неизвестно** и **вызывает сомнения**). Эти результаты показывают лишь **наличие способности**, а не то, «**захочет**» ли модель так поступать. Тем не менее сам **факт возможности** тревожит.

Учитывая «**опасности**» новой модели, **OpenBrain** «**ответственно**» решает **пока не выпускать** её публично (к тому же компания хочет сфокусироваться на **внутреннем AI R&D**).[^^27] Знание о **полных возможностях Agent-2** ограничено **узким силосом**: непосредственная команда, руководство и безопасность OpenBrain, несколько десятков чиновников правительства США и — после многолетней инфильтрации — **легионы шпионов КПК**.[^silo]

---

[^24]: Например, **через rejection sampling**, как в **DeepSeek R1** и **Llama 3.1** от Meta.

[^25]: Порядка **20 000** эквивалентов полной занятости по **$100/ч × 2000 ч/год ≈ $4 млрд/год**.

[^26]: Речь о намерении значительно **ускорить собственные ключевые исследования**, используя **труд Agent-2**, чтобы быстрее **натренировать ещё лучшие ИИ**, что вызовет **дальнейшее ускорение** и т. д. Анализ «экстремальной» версии этой воронки (которую мы всё же считаем **поразительно правдоподобной**) см. в соответствующем отчёте.

[^taste]: Есть некоторые свидетельства, что ИИ уже обладают **базовым «исследовательским чутьём»**: см., например, работу об **ИИ-со-учёном** и работу о **генерации новых идей** для исследований, фокусирующихся на LLM. Команды с лучшим «вкусом» всё равно должны проводить эксперименты, но они **лучше выбирают**, какие эксперименты запускать и **какие уроки** извлекать из результатов.

[^silo]: В «силос» входят **200 исследователей OpenBrain** (10 руководителей, **140** из команды **capabilities**, **25** из подразделений **безопасности/мониторинга/контроля**, **15** из команд типа **Preparedness/RSP**, **10** из **alignment**) и **50 госчиновников** (15 из **Белого дома**, 5 из **AISI**, 10 из **DOD**, 10 из **DOE** и 10 из **CISA**). В проекте есть **несколько шпионов** с разной степенью добровольности; часто это **принуждённые** люди (например, у кого семьи в Китае), а не идейные соратники.

[^^27]: Кроме того, компания по-прежнему видит **большую отдачу** от дальнейшего пост-тренинга, и **нет особой причины** сейчас выпускать продукт. Нередко бывает **несколько месяцев задержки** между внутренними результатами и релизом модели — часть времени уходит на **red teaming**.

## Февраль 2027: Китай крадёт Agent-2

**OpenBrain** представляет **Agent-2** правительству США — в том числе **Совету национальной безопасности (NSC)**, **Министерству обороны (DoD)** и **Институту безопасности ИИ США (AISI)**. OpenBrain важно сохранять хорошие отношения с исполнительной властью: по сути, это единственный актор, который сейчас способен их остановить; к тому же, если Белый дом не держать в курсе, он всё равно может обо всём узнать — и остаться недоволен.[^28]

Чиновников больше всего интересуют **кибербоевые** возможности Agent-2: он «всего лишь» немного уступает лучшим человеческим хакерам, но **тысячи копий** можно запускать **параллельно**, находя и эксплуатируя уязвимости быстрее, чем защитники успевают реагировать. Для Пентагона это — **критическое преимущество** в кибервойне, и тема ИИ перемещается **с 5-го на 2-е** место в списке приоритетов администрации (а **№ 1** всегда — текущий кризис: например, наращивание вооружений вокруг Тайваня, новая война на Ближнем Востоке или внутренняя политическая турбулентность).

Кто-то предлагает **национализировать** OpenBrain, но другие члены кабинета считают, что пока рано. Сотрудник аппарата готовит записку с вариантами для президента — от «как обычно» до **полной национализации**. Президент прислушивается к советам — лидеры техиндустрии убеждают, что национализация «**убьёт гусыню, несущую золотые яйца**». Он решает **пока воздержаться** от резких шагов и лишь добавляет **дополнительные требования к безопасности** в контракт OpenBrain–DoD.

Слишком поздно. Руководство КПК понимает значимость **Agent-2** и отдаёт приказ разведке и киберподразделениям **украсть веса**. Рано утром агент на базе **Agent-1**, отслеживающий сетевой трафик, фиксирует **аномальную передачу данных**. Он оповещает руководство компании, те — **Белый дом**. Признаки операции **уровня государства** неоспоримы; кража усиливает ощущение **нарастающей гонки вооружений**.

> ➤ См. **Приложение D — кража весов модели Agent-2**.

Белый дом **укорочивает поводок** OpenBrain и усиливает их службу безопасности людьми из армии и разведсообщества. **Первый приоритет** — **не допустить новых краж весов**.[^conn]

В ответ на кражу президент **санкционирует кибератаки** для саботажа **DeepCent**. Но к этому моменту у Китая уже **~40%** своей **релевантной ИИ-вычислительной мощности**[^cdz] сосредоточено в **CDZ** — Централизованной зоне разработки, где безопасность агрессивно ужесточили: **air-gap** (отсечение внешних соединений) и **внутренние силосы**. Операции не наносят серьёзного **немедленного** ущерба. Напряжение растёт; стороны демонстрируют серьёзность, **перегруппировывая силы** вокруг Тайваня, а **DeepCent** лихорадочно пытается **запустить Agent-2 эффективно**, чтобы начать ускорять собственные AI-исследования.[^cn-ops]

---

[^28]: *Почему Конгресс не может их остановить? А суды?* Слишком медленно; слишком раздроблены и/или излишне уважительны к президенту; недостаточно информированы. Есть и **позитивные** причины сохранять хорошие отношения с Белым домом: он может **снимать бюрократию**, помогать с **безопасностью**, **предупреждать штатное регулирование**, а в отдельных случаях — даже **тормозить конкурентов**.

[^conn]: *Простейшая надёжная мера* — **закрыть все высокоскоростные внешние соединения** дата-центров компании, но это сделает передачу больших файлов практически невозможной. Поэтому внешние каналы **почти все** глушат, однако дата-центрам, **участвующим в обучении**, нужно **быстро обмениваться весами**; сильное ограничение этих каналов слишком замедлит прогресс. OpenBrain сохраняет такие связи, но с **усиленным мониторингом** и **дополнительным шифрованием**.

[^cdz]: Напомним: **с середины 2026 года** Китай направляет **80%** **новых** AI-чипов в CDZ. Поскольку совокупная мощность с начала 2026-го **удвоилась** (в русле мирового тренда), это даёт **~2 млн** GPU **уровня 2024 года (H100-экв.)** и **~2 ГВт** потребления. У OpenBrain по-прежнему **в 2 раза больше** вычислений, чем у DeepCent, а **все прочие** американские компании вместе взятые имеют **в 5 раз больше**, чем DeepCent. См. распределение в *compute-дополнении*.

[^cn-ops]: Несмотря на централизованный национальный курс, у DeepCent остаётся **небольшое, но важное** отставание по вычислениям. Помимо того, что у Китая примерно **половина** суммарной мощности, ему приходится использовать **больше чипов** в целом — они в среднем **хуже качеством**, а GPU **гетерогенны** (их не всегда легко эффективно связать), что нагружает сетевое взаимодействие «чип-к-чипу». Есть и **софтверные** отличия (например, на не-Nvidia-GPU нет **CUDA**) и различия в **аппаратных спецификациях**, из-за чего тренинг-код выходит **сложнее**, **медленнее** и **более аварийным**. Достичь **высокой утилизации** — отдельная задача: **ингест данных**, **планирование**, **коллективные коммуникации** и **алгоритмы параллелизма** отстают от американских. Однако большинство проблем решается **усилиями и тестированием** — а это как раз отличная работа для **только что похищенного Agent-2**; **спустя около месяц** аптайм проекта и **средняя утилизация ресурсов** (обучение и инференс) улучшаются — остаётся лишь **незначительное** отставание от США.

## Март 2027: алгоритмические прорывы

Благодаря прорывам в области возможностей **Agent-3** — быстрый и дешёвый сверхчеловеческий кодер. **OpenBrain** запускает **200 000** копий Agent-3 параллельно, формируя рабочую силу, эквивалентную **50 000** «копий» лучшего человеческого программиста, ускоренных в **30×**.[^star]

OpenBrain всё равно сохраняет в штате инженеров-людей: их навыки **дополняют** ИИ и нужны для **управления командами** копий Agent-3. Например, «**исследовательский вкус**» пока трудно обучать — обратные связи **длинные**, данных **мало**.[^30] Эта колоссальная сверхчеловеческая «рабсила» ускоряет **общий темп алгоритмического прогресса** OpenBrain «**всего лишь**» в **4×** — из-за **узких мест** и **убывающей отдачи** от дополнительного «кодерского» труда.[^31]

Теперь, когда **кодирование полностью автоматизировано**, OpenBrain может быстро **штамповать** высококачественные **обучающие среды**, чтобы натаскивать Agent-3 на его **слабые навыки** — вроде исследовательского вкуса и **крупномасштабной координации**. Если раньше учебные среды выглядели как: «Вот несколько **GPU** и инструкции к экспериментам, **закодируй и запусти**; твою работу оценят **как у ML-инженера**», то теперь они такие: «Вот **пара сотен GPU**, **интернет-соединение** и **исследовательские челленджи**; **ты и тысяча** других копий должны **вместе** продвинуть исследования. Чем впечатляющe результат, тем **выше** твой балл».

➤ См. **Приложение G — почему мы прогнозируем появление сверхчеловеческого кодера в начале 2027 года**.

---

[^30]: Некоторые аспекты играют на **сильные стороны ИИ** — например, отдача от знания литературы по машинному обучению и **скорость/стоимость** генерации множества идей. Но эти плюсы **перевешиваются** слабостями.

[^31]: Почему только **4×**? Это наш **неуверенный** лучший прогноз, основанный на рассуждениях из **дополнения о «взлёте»**. Исторически примерно **половина** совокупного прогресса приходилась на **улучшение алгоритмов** (лучшие идеи, новые парадигмы), а **вторая половина** — на **масштабирование вычислений**. Поэтому **4×** ускорение **алгоритмического** прогресса соответствует примерно **2×** росту **общего** темпа прогресса.

[^star]: Мы ожидаем, что Agent-3 требует вычислений для инференса **порядка трансформера на ~10 трлн параметров**. Значит, выделив **~6%** своего вычислительного бюджета на запуск Agent-3, можно держать около **200 000 копий** на скорости мышления **30×** относительно человека (подробности — в разделе об автоматизации AI-исследований **compute-supplement**). Каждый «суперчеловеческий» кодерный **скаффолд** поверх Agent-3 в среднем эквивалентен примерно **восьми** «внутренним» копиям Agent-3 (на деле это может быть набор **меньших/специализированных моделей**, которым Agent-3 **делегирует** подзадачи).

#